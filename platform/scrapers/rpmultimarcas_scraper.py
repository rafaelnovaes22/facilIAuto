"""
Scraper para RP Multimarcas - Extra√ß√£o de dados de ve√≠culos
Site: https://rpmultimarcas.com.br/
Se√ß√£o: Nossos ve√≠culos

IMPORTANTE: Segue princ√≠pio "Nunca Invente Dados"
- Retorna None quando dado n√£o existe
- N√£o assume valores padr√£o
"""

import requests
from bs4 import BeautifulSoup
import json
import re
from datetime import datetime
from typing import List, Dict, Optional
import time
import hashlib


class RPMultimarcasScraper:
    """Scraper para o site RP Multimarcas"""
    
    def __init__(self):
        self.base_url = "https://rpmultimarcas.com.br"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def extract_price(self, text: str) -> Optional[float]:
        """
        Extrair pre√ßo do texto
        Retorna None se n√£o encontrar
        """
        if not text:
            return None
        
        # Remover "R$" e espa√ßos
        text = text.replace('R$', '').replace(' ', '').strip()
        
        # Padr√£o: 95.990,00 ou 95990
        pattern = r'(\d+(?:\.\d+)*(?:,\d+)?)'
        match = re.search(pattern, text)
        
        if match:
            price_str = match.group(1)
            # Normalizar: remover pontos, trocar v√≠rgula por ponto
            price_str = price_str.replace('.', '').replace(',', '.')
            
            try:
                price = float(price_str)
                if 10000 <= price <= 500000:
                    return price
            except ValueError:
                pass
        
        return None
    
    def extract_km(self, text: str) -> Optional[int]:
        """
        Extrair quilometragem do texto
        Retorna None se n√£o encontrar
        """
        if not text:
            return None
        
        text_lower = text.lower()
        
        # Zero km
        if 'zero km' in text_lower or '0 km' in text_lower or '0km' in text_lower:
            return 0
        
        # Padr√£o: n√∫meros seguidos de "km"
        pattern = r'(\d+(?:[.,]\d+)*)\s*(?:mil\s+)?km'
        match = re.search(pattern, text_lower)
        
        if match:
            km_str = match.group(1)
            km_str = km_str.replace('.', '').replace(',', '')
            
            try:
                km = int(km_str)
                
                # Se tem "mil" no texto, multiplicar por 1000
                if 'mil' in text_lower and km < 1000:
                    km *= 1000
                
                if 0 <= km <= 500000:
                    return km
            except ValueError:
                pass
        
        return None
    
    def extract_year(self, text: str) -> Optional[int]:
        """
        Extrair ano do texto
        Retorna None se n√£o encontrar
        """
        if not text:
            return None
        
        # Padr√£o: YYYY/YYYY ou YYYY
        pattern = r'(\d{4})(?:/(\d{4}))?'
        match = re.search(pattern, text)
        
        if match:
            # Se tem formato YYYY/YYYY, pegar o segundo (ano modelo)
            year = int(match.group(2) if match.group(2) else match.group(1))
            
            if 2010 <= year <= 2026:
                return year
        
        return None
    
    def extract_cambio(self, text: str) -> Optional[str]:
        """
        Extrair tipo de c√¢mbio do texto
        Retorna None se n√£o encontrar (N√ÉO assume "Manual")
        """
        if not text:
            return None
        
        text_lower = text.lower()
        
        # Padr√µes de autom√°tico
        if 'autom√°tico cvt' in text_lower or 'automatico cvt' in text_lower or 'cvt' in text_lower:
            return "Autom√°tico CVT"
        elif 'autom√°tico' in text_lower or 'automatico' in text_lower or 'automatic' in text_lower:
            return "Autom√°tico"
        elif 'automatizada' in text_lower or 'amt' in text_lower:
            return "Automatizada"
        elif 'manual' in text_lower:
            return "Manual"
        
        # Padr√µes curtos
        if re.search(r'\bA\b', text):
            return "Autom√°tico"
        elif re.search(r'\bM\b', text):
            return "Manual"
        
        return None  # N√£o assumir valor padr√£o
    
    def extract_combustivel(self, text: str) -> Optional[str]:
        """
        Extrair tipo de combust√≠vel
        Retorna None se n√£o encontrar
        """
        if not text:
            return None
        
        text_lower = text.lower()
        
        if 'flex' in text_lower:
            return "Flex"
        elif 'gasolina' in text_lower:
            return "Gasolina"
        elif 'diesel' in text_lower:
            return "Diesel"
        elif 'el√©trico' in text_lower or 'eletrico' in text_lower:
            return "El√©trico"
        elif 'h√≠brido' in text_lower or 'hibrido' in text_lower:
            return "H√≠brido"
        
        return None
    
    def extract_categoria(self, text: str) -> Optional[str]:
        """
        Extrair categoria do ve√≠culo
        Retorna None se n√£o encontrar
        """
        if not text:
            return None
        
        text_lower = text.lower()
        
        if 'suv' in text_lower:
            return "SUV"
        elif 'sedan' in text_lower:
            return "Sedan"
        elif 'hatch' in text_lower:
            return "Hatch"
        elif 'pickup' in text_lower or 'picape' in text_lower:
            return "Pickup"
        elif 'van' in text_lower:
            return "Van"
        elif 'compacto' in text_lower:
            return "Compacto"
        
        return None
    
    def calculate_content_hash(self, data: Dict) -> str:
        """Calcular hash MD5 do conte√∫do para detec√ß√£o de mudan√ßas"""
        hashable_data = {
            k: v for k, v in data.items()
            if k not in ['id', 'data_scraping', 'content_hash']
        }
        content_str = json.dumps(hashable_data, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(content_str.encode('utf-8')).hexdigest()
    
    def extract_car_details(self, car_url: str) -> Optional[Dict]:
        """
        Extrair detalhes de um carro espec√≠fico
        """
        try:
            print(f"      Acessando: {car_url}")
            response = self.session.get(car_url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            car_data = {
                'url_original': car_url,
                'data_scraping': datetime.now().isoformat()
            }
            
            # Tentar extrair nome/t√≠tulo
            # Seletores comuns: h1, .title, .car-title, .vehicle-title
            title = (soup.find('h1') or 
                    soup.find(class_='title') or 
                    soup.find(class_='car-title') or
                    soup.find(class_='vehicle-title'))
            
            if title:
                car_data['nome'] = title.text.strip()
            
            # Tentar extrair pre√ßo
            # Seletores comuns: .price, .valor, .preco
            price = (soup.find(class_='price') or 
                    soup.find(class_='valor') or 
                    soup.find(class_='preco'))
            
            if price:
                price_value = self.extract_price(price.text)
                if price_value:
                    car_data['preco'] = price_value
            
            # Tentar extrair caracter√≠sticas
            # Procurar por lista de especifica√ß√µes
            specs = soup.find_all(['li', 'div'], class_=re.compile(r'spec|feature|caracteristica'))
            
            for spec in specs:
                text = spec.text.strip()
                
                # Tentar identificar o tipo de informa√ß√£o
                if 'ano' in text.lower() or 'fabrica√ß√£o' in text.lower():
                    year = self.extract_year(text)
                    if year:
                        car_data['ano'] = year
                
                elif 'km' in text.lower() or 'quilometragem' in text.lower():
                    km = self.extract_km(text)
                    if km is not None:
                        car_data['quilometragem'] = km
                
                elif 'c√¢mbio' in text.lower() or 'cambio' in text.lower() or 'transmiss√£o' in text.lower():
                    cambio = self.extract_cambio(text)
                    if cambio:
                        car_data['cambio'] = cambio
                
                elif 'combust√≠vel' in text.lower() or 'combustivel' in text.lower():
                    combustivel = self.extract_combustivel(text)
                    if combustivel:
                        car_data['combustivel'] = combustivel
                
                elif 'cor' in text.lower():
                    # Extrair cor (texto ap√≥s "Cor:")
                    cor_match = re.search(r'cor:?\s*(.+)', text, re.IGNORECASE)
                    if cor_match:
                        car_data['cor'] = cor_match.group(1).strip()
                
                elif 'porta' in text.lower():
                    # Extrair n√∫mero de portas
                    portas_match = re.search(r'(\d+)\s*porta', text, re.IGNORECASE)
                    if portas_match:
                        car_data['portas'] = int(portas_match.group(1))
            
            # Tentar extrair imagens
            images = soup.find_all('img', src=re.compile(r'\.(jpg|jpeg|png|webp)', re.IGNORECASE))
            car_images = []
            for img in images:
                src = img.get('src', '')
                # Filtrar imagens de ve√≠culos (geralmente cont√©m 'car', 'vehicle', 'veiculo' no path)
                if src and ('car' in src.lower() or 'vehicle' in src.lower() or 'veiculo' in src.lower() or 'upload' in src.lower()):
                    if src.startswith('http'):
                        car_images.append(src)
                    elif src.startswith('/'):
                        car_images.append(self.base_url + src)
            
            if car_images:
                car_data['imagens'] = car_images[:10]  # M√°ximo 10 imagens
            
            # Tentar extrair descri√ß√£o
            description = (soup.find(class_='description') or 
                          soup.find(class_='descricao') or
                          soup.find('div', class_=re.compile(r'desc')))
            
            if description:
                car_data['descricao'] = description.text.strip()[:5000]  # M√°ximo 5000 caracteres
            
            # Calcular hash
            car_data['content_hash'] = self.calculate_content_hash(car_data)
            
            return car_data
            
        except Exception as e:
            print(f"      ‚ùå Erro ao extrair detalhes: {e}")
            return None
    
    def scrape_listing_page(self) -> List[str]:
        """
        Extrair URLs de carros da p√°gina principal
        """
        try:
            print(f"\nüìÑ Acessando p√°gina de listagem...")
            response = self.session.get(self.base_url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Procurar por links de ve√≠culos
            # Seletores comuns: a.car-link, a.vehicle-link, links dentro de .vehicle-card
            car_links = []
            
            # Estrat√©gia 1: Links com classes espec√≠ficas
            links = soup.find_all('a', class_=re.compile(r'car|vehicle|veiculo', re.IGNORECASE))
            car_links.extend(links)
            
            # Estrat√©gia 2: Links dentro de cards/containers de ve√≠culos
            containers = soup.find_all(['div', 'article'], class_=re.compile(r'car|vehicle|veiculo|card', re.IGNORECASE))
            for container in containers:
                link = container.find('a', href=True)
                if link:
                    car_links.append(link)
            
            # Extrair URLs √∫nicas
            urls = set()
            for link in car_links:
                href = link.get('href', '')
                if href:
                    # Garantir URL completa
                    if href.startswith('http'):
                        urls.add(href)
                    elif href.startswith('/'):
                        urls.add(self.base_url + href)
                    else:
                        urls.add(self.base_url + '/' + href)
            
            # Filtrar URLs que parecem ser de ve√≠culos
            vehicle_urls = [url for url in urls if any(keyword in url.lower() for keyword in ['veiculo', 'vehicle', 'car', 'estoque'])]
            
            return list(vehicle_urls)
            
        except Exception as e:
            print(f"‚ùå Erro ao extrair listagem: {e}")
            return []
    
    def scrape_all(self) -> List[Dict]:
        """
        Fazer scraping de todos os ve√≠culos
        """
        all_cars = []
        
        print(f"\nüîç Iniciando scraping do RP Multimarcas...")
        print(f"Site: {self.base_url}")
        
        # Obter URLs dos ve√≠culos
        car_urls = self.scrape_listing_page()
        
        if not car_urls:
            print("‚ö†Ô∏è  Nenhum ve√≠culo encontrado na listagem")
            print("   Isso pode significar que:")
            print("   1. O site usa JavaScript para carregar ve√≠culos")
            print("   2. Os seletores CSS precisam ser ajustados")
            print("   3. O site est√° temporariamente indispon√≠vel")
            return []
        
        print(f"   Encontrados {len(car_urls)} ve√≠culos")
        
        # Extrair detalhes de cada ve√≠culo
        for i, car_url in enumerate(car_urls, 1):
            print(f"\n   [{i}/{len(car_urls)}] Processando ve√≠culo...")
            
            car_data = self.extract_car_details(car_url)
            
            if car_data:
                # Validar campos obrigat√≥rios
                required_fields = ['nome', 'preco', 'ano', 'quilometragem']
                missing_fields = [f for f in required_fields if f not in car_data]
                
                if missing_fields:
                    print(f"      ‚ö†Ô∏è  Campos obrigat√≥rios faltando: {missing_fields}")
                    print(f"      ‚ö†Ô∏è  Ve√≠culo ser√° rejeitado")
                else:
                    all_cars.append(car_data)
                    print(f"      ‚úÖ {car_data.get('nome', 'N/A')}")
                    print(f"         Pre√ßo: R$ {car_data.get('preco', 0):,.2f}")
                    print(f"         Ano: {car_data.get('ano', 'N/A')}")
                    print(f"         KM: {car_data.get('quilometragem', 'N/A'):,}")
            
            # Delay para n√£o sobrecarregar o servidor
            time.sleep(2)
        
        print(f"\n‚úÖ Scraping conclu√≠do: {len(all_cars)} carros v√°lidos de {len(car_urls)} encontrados")
        
        return all_cars
    
    def save_to_json(self, cars: List[Dict], filename: str):
        """
        Salvar dados em JSON
        """
        if not cars:
            print("\n‚ö†Ô∏è  Nenhum carro para salvar")
            return
        
        # Adicionar metadata
        output = {
            'metadata': {
                'source': 'rpmultimarcas.com.br',
                'scraper_version': '1.0.0',
                'timestamp': datetime.now().isoformat(),
                'total_vehicles': len(cars)
            },
            'vehicles': cars
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ Dados salvos em: {filename}")
        print(f"   Total de ve√≠culos: {len(cars)}")


def main():
    """Fun√ß√£o principal"""
    scraper = RPMultimarcasScraper()
    
    # Fazer scraping
    cars = scraper.scrape_all()
    
    # Salvar
    if cars:
        scraper.save_to_json(cars, 'rpmultimarcas_estoque.json')
    else:
        print("\n‚ùå Nenhum carro extra√≠do")
        print("\nüí° Dica: O site pode usar JavaScript para carregar ve√≠culos.")
        print("   Neste caso, ser√° necess√°rio:")
        print("   1. Usar Selenium/Playwright para renderizar JavaScript")
        print("   2. Ou extrair dados manualmente e importar via CSV")


if __name__ == "__main__":
    main()
