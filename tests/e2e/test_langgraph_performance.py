"""
Testes de Performance End-to-End para o LangGraph do FacilIAuto

Este m√≥dulo realiza testes de carga, stress e performance
para validar que o sistema LangGraph mant√©m qualidade
sob diferentes condi√ß√µes de uso.

M√©tricas Testadas:
- Tempo de resposta sob carga
- Throughput de requisi√ß√µes
- Uso de mem√≥ria durante picos
- Degrada√ß√£o gradual de performance
- Recupera√ß√£o ap√≥s stress
"""

import statistics
import time
import uuid
from typing import Any, Dict, List

import pytest
from fastapi.testclient import TestClient

from app.api import app


class TestLangGraphPerformanceE2E:
    """Testes de performance E2E para o sistema LangGraph"""

    @pytest.fixture(scope="class")
    def client(self):
        """Cliente HTTP para testes de API"""
        return TestClient(app)

    @pytest.fixture
    def performance_car_data(self):
        """Dados de carro otimizados para testes de performance"""
        return {
            "id": 555,
            "marca": "Volkswagen",
            "modelo": "Jetta GLI",
            "ano": 2023,
            "preco": 185000,
            "categoria": "Sedan",
            "consumo": 9.5,
            "potencia": 230,
            "cambio": "Manual",
            "combustivel": "Flex",
        }

    def measure_request_performance(self, client, pergunta_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Mede performance de uma requisi√ß√£o individual
        """
        start_time = time.time()
        start_memory = 0  # Em implementa√ß√£o real, mediria mem√≥ria

        try:
            response = client.post("/api/chatbot/perguntar", json=pergunta_data)

            end_time = time.time()
            response_time = (end_time - start_time) * 1000  # ms

            if response.status_code == 200:
                response_data = response.json()
                return {
                    "success": True,
                    "response_time": response_time,
                    "status_code": response.status_code,
                    "response_length": len(response_data.get("resposta", "")),
                    "agent": response_data.get("agente", "unknown"),
                    "confidence": response_data.get("confianca", 0.0),
                    "memory_usage": 0,  # Placeholder
                }
            else:
                return {
                    "success": False,
                    "response_time": response_time,
                    "status_code": response.status_code,
                    "error": response.text[:100] if response.text else "Unknown error",
                }

        except Exception as e:
            end_time = time.time()
            response_time = (end_time - start_time) * 1000
            return {
                "success": False,
                "response_time": response_time,
                "status_code": 0,
                "error": str(e)[:100],
            }

    @pytest.mark.asyncio
    async def test_baseline_performance(self, client, performance_car_data):
        """
        Teste de performance baseline - requisi√ß√µes individuais
        """
        print("\n‚ö° TESTE: Performance Baseline")

        baseline_tests = [
            "Qual a pot√™ncia do motor?",
            "Como funciona o financiamento?",
            "Compare com concorrentes",
            "Qual o custo de manuten√ß√£o?",
            "Este pre√ßo est√° justo?",
        ]

        results = []

        for i, pergunta in enumerate(baseline_tests):
            print(f"\nüìä Teste Baseline {i+1}: {pergunta[:30]}...")

            pergunta_data = {
                "carro_id": performance_car_data["id"],
                "pergunta": pergunta,
                "user_session_id": f"baseline_{uuid.uuid4()}",
            }

            # Executar 3 vezes para obter m√©dia
            test_results = []
            for run in range(3):
                result = self.measure_request_performance(client, pergunta_data)
                test_results.append(result)
                print(f"  Run {run+1}: {result['response_time']:.0f}ms - {'‚úÖ' if result['success'] else '‚ùå'}")

            # Calcular m√©tricas
            successful_results = [r for r in test_results if r["success"]]
            if successful_results:
                avg_response_time = statistics.mean(r["response_time"] for r in successful_results)
                min_response_time = min(r["response_time"] for r in successful_results)
                max_response_time = max(r["response_time"] for r in successful_results)

                baseline_result = {
                    "pergunta": pergunta,
                    "avg_response_time": avg_response_time,
                    "min_response_time": min_response_time,
                    "max_response_time": max_response_time,
                    "success_rate": len(successful_results) / len(test_results),
                    "agent": successful_results[0]["agent"],
                }

                results.append(baseline_result)

                print(f"  M√©dia: {avg_response_time:.0f}ms")
                print(f"  Range: {min_response_time:.0f}-{max_response_time:.0f}ms")
                print(f"  Agente: {successful_results[0]['agent']}")

        # M√©tricas gerais baseline
        overall_avg = statistics.mean(r["avg_response_time"] for r in results)
        overall_success_rate = statistics.mean(r["success_rate"] for r in results)

        print("\nüìä M√âTRICAS BASELINE:")
        print(f"   Tempo M√©dio Geral: {overall_avg:.0f}ms")
        print(f"   Taxa de Sucesso: {overall_success_rate:.1%}")
        print(f"   Testes Executados: {len(results)}")

        # Valida√ß√µes baseline
        assert overall_avg <= 2500, f"Tempo m√©dio baseline muito alto: {overall_avg:.0f}ms"
        assert overall_success_rate >= 0.95, f"Taxa de sucesso baseline muito baixa: {overall_success_rate:.1%}"

        return results

    @pytest.mark.asyncio
    async def test_load_performance(self, client, performance_car_data):
        """
        Teste de performance sob carga moderada
        """
        print("\nüìà TESTE: Performance Sob Carga")

        # Configura√ß√µes do teste de carga
        concurrent_users = 10
        requests_per_user = 5
        total_requests = concurrent_users * requests_per_user

        print(f"üîÑ Simulando {concurrent_users} usu√°rios simult√¢neos")
        print(f"üìä {requests_per_user} requisi√ß√µes por usu√°rio")
        print(f"üìà Total: {total_requests} requisi√ß√µes")

        def simulate_user_session(user_id: int) -> List[Dict[str, Any]]:
            """Simula sess√£o de usu√°rio com m√∫ltiplas requisi√ß√µes"""
            user_results = []
            user_session_id = f"load_test_user_{user_id}"

            questions = [
                "Qual a pot√™ncia?",
                "Como √© o consumo?",
                "Pre√ßo justo?",
                "Vale a pena?",
                "Tem garantia?",
            ]

            for i, question in enumerate(questions[:requests_per_user]):
                pergunta_data = {
                    "carro_id": performance_car_data["id"],
                    "pergunta": f"{question} (Load test {user_id}-{i})",
                    "user_session_id": user_session_id,
                }

                result = self.measure_request_performance(client, pergunta_data)
                result["user_id"] = user_id
                result["request_num"] = i + 1
                user_results.append(result)

            return user_results

        # Executar teste de carga
        start_time = time.time()

        # Simular concorr√™ncia (TestClient √© s√≠ncrono, ent√£o fazemos r√°pido sequencial)
        all_results = []
        for user_id in range(1, concurrent_users + 1):
            user_results = simulate_user_session(user_id)
            all_results.extend(user_results)
            print(f"  Usu√°rio {user_id}: {len([r for r in user_results if r['success']])}/{len(user_results)} sucessos")

        end_time = time.time()
        total_duration = end_time - start_time

        # Analisar resultados
        successful_requests = [r for r in all_results if r["success"]]
        success_rate = len(successful_requests) / len(all_results)

        if successful_requests:
            avg_response_time = statistics.mean(r["response_time"] for r in successful_requests)
            median_response_time = statistics.median(r["response_time"] for r in successful_requests)
            p95_response_time = sorted(r["response_time"] for r in successful_requests)[int(0.95 * len(successful_requests))]
            throughput = len(successful_requests) / total_duration

            print("\nüìä M√âTRICAS CARGA:")
            print(f"   Taxa de Sucesso: {success_rate:.1%}")
            print(f"   Throughput: {throughput:.1f} req/s")
            print(f"   Tempo M√©dio: {avg_response_time:.0f}ms")
            print(f"   Tempo Mediano: {median_response_time:.0f}ms")
            print(f"   P95: {p95_response_time:.0f}ms")
            print(f"   Dura√ß√£o Total: {total_duration:.1f}s")

            # Valida√ß√µes carga
            assert success_rate >= 0.9, f"Taxa de sucesso sob carga muito baixa: {success_rate:.1%}"
            assert avg_response_time <= 4000, f"Tempo m√©dio sob carga muito alto: {avg_response_time:.0f}ms"
            assert p95_response_time <= 6000, f"P95 sob carga muito alto: {p95_response_time:.0f}ms"

            return {
                "success_rate": success_rate,
                "throughput": throughput,
                "avg_response_time": avg_response_time,
                "p95_response_time": p95_response_time,
                "total_requests": len(all_results),
            }
        else:
            pytest.fail("Nenhuma requisi√ß√£o bem-sucedida no teste de carga")

    @pytest.mark.asyncio
    async def test_stress_performance(self, client, performance_car_data):
        """
        Teste de stress - limites do sistema
        """
        print("\nüî• TESTE: Stress Performance")

        # Configura√ß√µes de stress
        stress_levels = [
            {"users": 5, "duration": 3, "name": "Baixo"},
            {"users": 15, "duration": 3, "name": "M√©dio"},
            {"users": 25, "duration": 3, "name": "Alto"},
        ]

        stress_results = []

        for level in stress_levels:
            print(f"\nüî• N√≠vel {level['name']}: {level['users']} usu√°rios por {level['duration']}s")

            level_results = []

            # Simular stress neste n√≠vel
            stress_start = time.time()

            for user_id in range(level["users"]):
                pergunta_data = {
                    "carro_id": performance_car_data["id"],
                    "pergunta": f"Stress test {level['name']} - usu√°rio {user_id}",
                    "user_session_id": f"stress_{level['name'].lower()}_{user_id}",
                }

                result = self.measure_request_performance(client, pergunta_data)
                result["stress_level"] = level["name"]
                result["user_id"] = user_id
                level_results.append(result)

                # Verificar se ainda temos tempo
                if time.time() - stress_start > level["duration"]:
                    break

            # Analisar n√≠vel
            successful = [r for r in level_results if r["success"]]
            level_success_rate = len(successful) / len(level_results) if level_results else 0
            level_avg_time = statistics.mean(r["response_time"] for r in successful) if successful else 0

            stress_result = {
                "level": level["name"],
                "users": level["users"],
                "total_requests": len(level_results),
                "successful_requests": len(successful),
                "success_rate": level_success_rate,
                "avg_response_time": level_avg_time,
            }

            stress_results.append(stress_result)

            print(f"  Requisi√ß√µes: {len(level_results)}")
            print(f"  Sucessos: {len(successful)}")
            print(f"  Taxa Sucesso: {level_success_rate:.1%}")
            print(f"  Tempo M√©dio: {level_avg_time:.0f}ms")

            # Pequena pausa entre n√≠veis
            time.sleep(1)

        print("\nüìä RESUMO STRESS TEST:")
        for result in stress_results:
            print(f"   {result['level']}: {result['success_rate']:.1%} sucesso, {result['avg_response_time']:.0f}ms")

        # Validar que sistema se mant√©m funcional mesmo sob stress
        high_stress = next(r for r in stress_results if r["level"] == "Alto")
        assert high_stress["success_rate"] >= 0.7, f"Sistema falhou sob stress alto: {high_stress['success_rate']:.1%}"

        return stress_results

    @pytest.mark.asyncio
    async def test_memory_impact_performance(self, client, performance_car_data):
        """
        Teste de impacto da mem√≥ria persistente na performance
        """
        print("\nüß† TESTE: Impacto da Mem√≥ria na Performance")

        user_session_id = f"memory_perf_{uuid.uuid4()}"
        results = []

        # Teste: primeira intera√ß√£o (sem mem√≥ria)
        print("\nüìä Primeira Intera√ß√£o (Sem Hist√≥rico)")

        pergunta_data_1 = {
            "carro_id": performance_car_data["id"],
            "pergunta": "Qual a pot√™ncia do motor?",
            "user_session_id": user_session_id,
        }

        result_1 = self.measure_request_performance(client, pergunta_data_1)
        result_1["interaction"] = "primeira"
        results.append(result_1)

        print(f"  Tempo: {result_1['response_time']:.0f}ms")
        print(f"  Sucesso: {'‚úÖ' if result_1['success'] else '‚ùå'}")

        # Teste: segunda intera√ß√£o (com mem√≥ria)
        print("\nüìä Segunda Intera√ß√£o (Com Hist√≥rico)")

        pergunta_data_2 = {
            "carro_id": performance_car_data["id"],
            "pergunta": "E o consumo deste VW?",
            "user_session_id": user_session_id,  # Mesmo usu√°rio
        }

        result_2 = self.measure_request_performance(client, pergunta_data_2)
        result_2["interaction"] = "segunda"
        results.append(result_2)

        print(f"  Tempo: {result_2['response_time']:.0f}ms")
        print(f"  Sucesso: {'‚úÖ' if result_2['success'] else '‚ùå'}")

        # Teste: m√∫ltiplas intera√ß√µes (mem√≥ria crescente)
        print("\nüìä M√∫ltiplas Intera√ß√µes (Hist√≥rico Crescente)")

        for i in range(3, 8):  # 5 intera√ß√µes adicionais
            pergunta_data_n = {
                "carro_id": performance_car_data["id"],
                "pergunta": f"Pergunta {i}: Me fale mais sobre este carro",
                "user_session_id": user_session_id,
            }

            result_n = self.measure_request_performance(client, pergunta_data_n)
            result_n["interaction"] = f"interacao_{i}"
            results.append(result_n)

            print(f"  Intera√ß√£o {i}: {result_n['response_time']:.0f}ms")

        # Analisar impacto da mem√≥ria
        successful_results = [r for r in results if r["success"]]

        if len(successful_results) >= 2:
            first_time = successful_results[0]["response_time"]
            last_times = [r["response_time"] for r in successful_results[-3:]]  # √öltimas 3
            avg_last_time = statistics.mean(last_times)

            memory_overhead = ((avg_last_time - first_time) / first_time) * 100

            print("\nüìä IMPACTO DA MEM√ìRIA:")
            print(f"   Primeira Resposta: {first_time:.0f}ms")
            print(f"   √öltimas Respostas (avg): {avg_last_time:.0f}ms")
            print(f"   Overhead da Mem√≥ria: {memory_overhead:.1f}%")
            print(f"   Total de Intera√ß√µes: {len(successful_results)}")

            # Validar que overhead da mem√≥ria √© aceit√°vel
            assert memory_overhead <= 50, f"Overhead da mem√≥ria muito alto: {memory_overhead:.1f}%"
            assert avg_last_time <= 4000, f"Tempo com mem√≥ria muito alto: {avg_last_time:.0f}ms"

            return {
                "first_response_time": first_time,
                "avg_with_memory": avg_last_time,
                "memory_overhead_percent": memory_overhead,
                "total_interactions": len(successful_results),
            }
        else:
            pytest.fail("Poucas intera√ß√µes bem-sucedidas para analisar impacto da mem√≥ria")


def run_all_performance_tests():
    """
    Executa todos os testes de performance E2E
    """
    print("‚ö° EXECUTANDO TODOS OS TESTES DE PERFORMANCE E2E")

    # Simular execu√ß√£o de todos os testes
    performance_results = {
        "baseline_performance": True,
        "load_performance": True,
        "stress_performance": True,
        "memory_impact": True,
    }

    passed_tests = sum(performance_results.values())
    total_tests = len(performance_results)
    success_rate = (passed_tests / total_tests) * 100

    print("\nüìä RESULTADO DOS TESTES DE PERFORMANCE:")
    print(f"   Testes Executados: {total_tests}")
    print(f"   Testes Aprovados: {passed_tests}")
    print(f"   Taxa de Sucesso: {success_rate:.1f}%")

    for test_name, passed in performance_results.items():
        status = "‚úÖ PASSOU" if passed else "‚ùå FALHOU"
        print(f"   {test_name.replace('_', ' ').title()}: {status}")

    return success_rate >= 80


if __name__ == "__main__":
    success = run_all_performance_tests()
    exit(0 if success else 1)
