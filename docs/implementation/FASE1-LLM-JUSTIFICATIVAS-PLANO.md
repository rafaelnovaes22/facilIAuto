# üìã Fase 1: SLM para Justificativas Inteligentes - Plano de Implementa√ß√£o

## üéØ Objetivo

Substituir a gera√ß√£o de justificativas baseada em templates por justificativas contextuais e precisas geradas por Small Language Model (SLM).

**Resultado esperado:** Cada recomenda√ß√£o de ve√≠culo ter√° uma explica√ß√£o natural, precisa e contextualizada que explica POR QU√ä aquele carro √© adequado para aquele usu√°rio espec√≠fico.

---

## üìä Escopo

### ‚úÖ O que vamos implementar

1. **Novo servi√ßo:** `LLMJustificationService` para gera√ß√£o de justificativas
2. **Integra√ß√£o:** Modificar `UnifiedRecommendationEngine` para usar o novo servi√ßo
3. **Fallback:** Manter l√≥gica atual como backup se LLM falhar
4. **Configura√ß√£o:** Suporte para m√∫ltiplos provedores (OpenAI, Anthropic, modelos locais)
5. **Testes:** Cobertura completa do novo servi√ßo
6. **Documenta√ß√£o:** Guia de configura√ß√£o e uso

### ‚ùå O que N√ÉO vamos implementar (ainda)

- An√°lise de contexto din√¢mica (Fase 2)
- Ajuste de pesos por LLM (Fase 2)
- LangGraph para refinamento (Fase 3)
- Aprendizado com feedback (Fase 3)

---

## üèóÔ∏è Arquitetura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  UnifiedRecommendationEngine                ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  recommend() ‚Üí filter ‚Üí score ‚Üí rank                       ‚îÇ
‚îÇ                                     ‚Üì                       ‚îÇ
‚îÇ                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ                            ‚îÇ Generate       ‚îÇ              ‚îÇ
‚îÇ                            ‚îÇ Justification  ‚îÇ              ‚îÇ
‚îÇ                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ                                     ‚Üì                       ‚îÇ
‚îÇ                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ                     ‚îÇ LLMJustificationService   ‚îÇ          ‚îÇ
‚îÇ                     ‚îÇ                           ‚îÇ          ‚îÇ
‚îÇ                     ‚îÇ ‚Ä¢ build_prompt()          ‚îÇ          ‚îÇ
‚îÇ                     ‚îÇ ‚Ä¢ call_llm()              ‚îÇ          ‚îÇ
‚îÇ                     ‚îÇ ‚Ä¢ validate_output()       ‚îÇ          ‚îÇ
‚îÇ                     ‚îÇ ‚Ä¢ cache_response()        ‚îÇ          ‚îÇ
‚îÇ                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                                 ‚Üì                           ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
‚îÇ                    ‚îÇ  LLM Provider           ‚îÇ             ‚îÇ
‚îÇ                    ‚îÇ                         ‚îÇ             ‚îÇ
‚îÇ                    ‚îÇ ‚Ä¢ OpenAI (GPT-4o-mini)  ‚îÇ             ‚îÇ
‚îÇ                    ‚îÇ ‚Ä¢ Anthropic (Haiku)     ‚îÇ             ‚îÇ
‚îÇ                    ‚îÇ ‚Ä¢ Local (Llama 3.1 8B)  ‚îÇ             ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚Üì
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Fallback Strategy      ‚îÇ
                    ‚îÇ                         ‚îÇ
                    ‚îÇ Se LLM falhar:          ‚îÇ
                    ‚îÇ ‚Üí usa justificativa     ‚îÇ
                    ‚îÇ   template atual        ‚îÇ
                    ‚îÇ ‚Üí log erro              ‚îÇ
                    ‚îÇ ‚Üí continua opera√ß√£o     ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìÅ Arquivos a Criar/Modificar

### Novos Arquivos

#### 1. `platform/backend/services/llm_justification_service.py`
```python
"""
Servi√ßo para gera√ß√£o de justificativas usando LLM
"""
from typing import Dict, Optional, List
from models.car import Car
from models.user_profile import UserProfile
import os
from openai import OpenAI
from anthropic import Anthropic
import logging

class LLMJustificationService:
    """Gera justificativas contextuais para recomenda√ß√µes usando LLM"""

    def __init__(
        self,
        provider: str = "openai",  # "openai", "anthropic", "local"
        model: str = None,
        api_key: str = None,
        cache_enabled: bool = True
    ):
        """
        Inicializa servi√ßo de justificativas LLM

        Args:
            provider: Provedor do LLM
            model: Nome do modelo (default usa modelo otimizado por provedor)
            api_key: Chave API (default usa vari√°vel de ambiente)
            cache_enabled: Habilita cache de respostas
        """
        pass

    def generate_justification(
        self,
        car: Car,
        profile: UserProfile,
        score: float,
        position: int,
        total_results: int,
        tco_breakdown: Dict,
        other_cars: Optional[List[Car]] = None
    ) -> str:
        """
        Gera justificativa contextual para uma recomenda√ß√£o

        Args:
            car: Carro recomendado
            profile: Perfil do usu√°rio
            score: Score final do matching
            position: Posi√ß√£o no ranking (1-based)
            total_results: Total de resultados
            tco_breakdown: Detalhamento do TCO
            other_cars: Outros carros no ranking (para compara√ß√£o)

        Returns:
            Justificativa em portugu√™s (2-3 frases)
        """
        pass

    def _build_prompt(
        self,
        car: Car,
        profile: UserProfile,
        score: float,
        position: int,
        total_results: int,
        tco_breakdown: Dict
    ) -> str:
        """Constr√≥i prompt otimizado para o LLM"""
        pass

    def _call_llm(self, prompt: str) -> str:
        """Chama o LLM com retry e error handling"""
        pass

    def _validate_output(self, text: str) -> bool:
        """Valida se a sa√≠da do LLM √© adequada"""
        pass

    def _get_cache_key(self, car: Car, profile: UserProfile) -> str:
        """Gera chave de cache"""
        pass
```

**Tamanho estimado:** ~300-400 linhas

---

#### 2. `platform/backend/services/llm_prompts.py`
```python
"""
Templates de prompts para LLM
"""

SYSTEM_PROMPT = """
Voc√™ √© um assistente especialista em recomenda√ß√£o de ve√≠culos.
Sua fun√ß√£o √© explicar de forma clara, honesta e contextual por que
um ve√≠culo espec√≠fico √© adequado para um usu√°rio.

REGRAS:
- Use 2-3 frases curtas e diretas
- Seja espec√≠fico com n√∫meros (pre√ßo, consumo, TCO)
- Compare com or√ßamento do usu√°rio quando relevante
- Mencione apenas pontos realmente importantes
- Seja honesto sobre limita√ß√µes (ex: "consumo moderado" n√£o "excelente")
- Use portugu√™s brasileiro coloquial mas profissional
- N√ÉO use emojis ou formata√ß√£o markdown
- N√ÉO invente dados que n√£o foram fornecidos
- N√ÉO seja gen√©rico ("√≥timo ve√≠culo"), seja espec√≠fico
"""

USER_PROMPT_TEMPLATE = """
Explique por que este carro √© recomendado para este usu√°rio:

CARRO:
- Modelo: {car_nome}
- Pre√ßo: R$ {car_preco:,}
- Categoria: {car_categoria}
- Ano: {car_ano}
- Consumo: {car_consumo_cidade} km/L (cidade), {car_consumo_estrada} km/L (estrada)
- Itens de Seguran√ßa: {car_seguranca}
- Itens de Conforto: {car_conforto}
- Concession√°ria: {dealership_nome} em {dealership_cidade}

USU√ÅRIO:
- Uso principal: {uso_principal}
- Tamanho da fam√≠lia: {tamanho_familia} pessoas
- Tem crian√ßas: {tem_criancas}
- Or√ßamento: R$ {orcamento_min:,} - R$ {orcamento_max:,}
- Prioridades: {prioridades_str}
- Renda mensal: R$ {renda_mensal:,}

AN√ÅLISE:
- Score de matching: {score_percent}%
- Ranking: #{position} de {total_results} recomenda√ß√µes
- TCO mensal estimado: R$ {tco_mensal:,} ({tco_percent}% da renda)
- Principais for√ßas: {top_features}

Gere uma explica√ß√£o contextual (2-3 frases).
"""

# Prompts espec√≠ficos por uso
COMMERCIAL_FOCUS_PROMPT = """
ATEN√á√ÉO: Este √© uso COMERCIAL. Foque em:
- Capacidade de carga / passageiros
- Custo operacional (TCO baixo √© cr√≠tico)
- Robustez e confiabilidade
- Adequa√ß√£o para a opera√ß√£o descrita
"""

FAMILY_FOCUS_PROMPT = """
ATEN√á√ÉO: Este √© uso FAMILIAR. Foque em:
- Espa√ßo interno e porta-malas
- Seguran√ßa (airbags, ISOFIX, estrutura)
- Conforto para viagens
- Economia (se prioridade alta)
"""

FIRST_CAR_FOCUS_PROMPT = """
ATEN√á√ÉO: Este √© o PRIMEIRO CARRO. Foque em:
- Facilidade de dirigir e estacionar
- Custo de manuten√ß√£o acess√≠vel
- Boa revenda futura
- Seguros mais baratos
"""
```

**Tamanho estimado:** ~200 linhas

---

#### 3. `platform/backend/tests/test_llm_justification_service.py`
```python
"""
Testes para LLMJustificationService
"""
import pytest
from services.llm_justification_service import LLMJustificationService
from models.car import Car
from models.user_profile import UserProfile

class TestLLMJustificationService:
    """Testes do servi√ßo de justificativas LLM"""

    def test_generate_justification_basic(self):
        """Testa gera√ß√£o b√°sica de justificativa"""
        pass

    def test_generate_justification_family_context(self):
        """Testa justificativa para contexto familiar"""
        pass

    def test_generate_justification_commercial_context(self):
        """Testa justificativa para contexto comercial"""
        pass

    def test_fallback_on_llm_failure(self):
        """Testa fallback quando LLM falha"""
        pass

    def test_cache_functionality(self):
        """Testa se cache est√° funcionando"""
        pass

    def test_prompt_building(self):
        """Testa constru√ß√£o de prompts"""
        pass

    def test_output_validation(self):
        """Testa valida√ß√£o de sa√≠da do LLM"""
        pass

    def test_different_providers(self):
        """Testa diferentes provedores (OpenAI, Anthropic)"""
        pass
```

**Tamanho estimado:** ~300-400 linhas

---

### Arquivos a Modificar

#### 1. `platform/backend/services/unified_recommendation_engine.py`

**Modifica√ß√µes:**

```python
# Linha ~60: Adicionar import
from services.llm_justification_service import LLMJustificationService

# Linha ~110: Adicionar no __init__
def __init__(self, data_dir: str = None, use_llm: bool = True):
    # ... c√≥digo existente ...

    # Inicializar LLM service
    self.use_llm = use_llm
    if use_llm:
        try:
            self.llm_service = LLMJustificationService()
            print("[ENGINE] LLM justification service habilitado")
        except Exception as e:
            print(f"[ENGINE] Erro ao inicializar LLM service: {e}")
            print("[ENGINE] Usando justificativas template como fallback")
            self.llm_service = None
    else:
        self.llm_service = None

# Linha ~1340: Modificar generate_justification
def _generate_justification(
    self,
    car: Car,
    profile: UserProfile,
    final_score: float,
    car_scores: dict,
    category_score: float,
    tco_info: dict,
    position: int = 1,
    total_results: int = 5
) -> str:
    """Gera justificativa para a recomenda√ß√£o"""

    # Tentar usar LLM primeiro
    if self.llm_service:
        try:
            return self.llm_service.generate_justification(
                car=car,
                profile=profile,
                score=final_score,
                position=position,
                total_results=total_results,
                tco_breakdown=tco_info.get("breakdown", {})
            )
        except Exception as e:
            print(f"[ENGINE] Erro ao gerar justificativa LLM: {e}")
            print("[ENGINE] Usando fallback template")

    # Fallback: l√≥gica template existente
    return self._generate_justification_template(
        car, profile, final_score, car_scores, category_score, tco_info
    )

# Nova fun√ß√£o: extrair l√≥gica template atual
def _generate_justification_template(
    self,
    car: Car,
    profile: UserProfile,
    final_score: float,
    car_scores: dict,
    category_score: float,
    tco_info: dict
) -> str:
    """
    Gera√ß√£o de justificativa usando templates (fallback)
    [Mover c√≥digo existente da linha 1340-1400 para c√°]
    """
    # ... c√≥digo existente de gera√ß√£o de justificativas ...
    pass
```

**Linhas afetadas:** ~1340-1400 (refatora√ß√£o + nova fun√ß√£o)

---

#### 2. `platform/backend/requirements.txt`

**Adicionar:**
```txt
# LLM Providers
openai>=1.12.0
anthropic>=0.18.0

# Caching (opcional, para otimiza√ß√£o)
redis>=5.0.0
```

---

#### 3. `platform/backend/.env.example`

**Adicionar:**
```bash
# LLM Configuration (Fase 1)
LLM_PROVIDER=openai  # openai, anthropic, local
LLM_MODEL=gpt-4o-mini  # ou claude-3-haiku-20240307
OPENAI_API_KEY=your_openai_key_here
ANTHROPIC_API_KEY=your_anthropic_key_here
LLM_CACHE_ENABLED=true
LLM_TIMEOUT_SECONDS=10
```

---

#### 4. `platform/backend/api/main.py`

**Modifica√ß√µes m√≠nimas:**

```python
# Linha ~76: Adicionar configura√ß√£o
use_llm = os.getenv("LLM_ENABLED", "true").lower() == "true"

try:
    print("[STARTUP] Carregando UnifiedRecommendationEngine...")
    engine = UnifiedRecommendationEngine(
        data_dir=data_dir,
        use_llm=use_llm  # ‚Üê Novo par√¢metro
    )
    print(f"[STARTUP] Engine carregado com {len(engine.all_cars)} carros")
    # ...
```

---

## üé® Prompt Engineering

### Prompt System (instru√ß√£o base)

```
Voc√™ √© um assistente especialista em recomenda√ß√£o de ve√≠culos.
Sua fun√ß√£o √© explicar de forma clara, honesta e contextual por que
um ve√≠culo espec√≠fico √© adequado para um usu√°rio.

REGRAS:
- Use 2-3 frases curtas e diretas
- Seja espec√≠fico com n√∫meros (pre√ßo, consumo, TCO)
- Compare com or√ßamento do usu√°rio quando relevante
- Mencione apenas pontos realmente importantes
- Seja honesto sobre limita√ß√µes
- Use portugu√™s brasileiro coloquial mas profissional
- N√ÉO use emojis ou formata√ß√£o markdown
- N√ÉO invente dados
- N√ÉO seja gen√©rico
```

### Exemplo de Input/Output

**Input:**
```
Carro: Volkswagen Taos 1.4 TSI 2023 - R$ 85.000
Categoria: SUV Compacto
Consumo: 11 km/L cidade, 13.5 km/L estrada
Seguran√ßa: 6 airbags, ABS, Controle de estabilidade, ISOFIX
Conforto: Ar-condicionado, Dire√ß√£o el√©trica, Vidros el√©tricos

Usu√°rio:
- Uso: fam√≠lia (4 pessoas, 2 crian√ßas)
- Or√ßamento: R$ 60.000 - R$ 100.000
- Prioridades: economia=5, seguran√ßa=5, espa√ßo=4
- Renda: R$ 8.000/m√™s

An√°lise:
- Score: 88%
- Ranking: #1 de 5
- TCO mensal: R$ 1.900 (24% da renda)
- Principais for√ßas: Seguran√ßa, Espa√ßo, Pre√ßo adequado
```

**Output esperado:**
```
O Volkswagen Taos √© a melhor op√ß√£o para sua fam√≠lia de 4 pessoas,
oferecendo amplo espa√ßo interno e 6 airbags com ISOFIX para as crian√ßas.
Com TCO de R$ 1.900/m√™s (24% da sua renda) e consumo moderado de 11 km/L,
fica confortavelmente dentro do seu or√ßamento de at√© R$ 100 mil.
```

### Exemplos por Contexto

**Fam√≠lia com crian√ßas:**
```
"O Honda HR-V combina espa√ßo (porta-malas de 437L) e seguran√ßa
(6 airbags + ISOFIX) ideal para sua fam√≠lia. O consumo de 12.5 km/L
e TCO de R$ 1.650/m√™s (21% da renda) permitem viagens sem peso no bolso."
```

**Comercial/Entrega:**
```
"O Fiat Fiorino atende perfeitamente opera√ß√µes de entrega urbana,
com capacidade de 650kg e custo operacional baix√≠ssimo (TCO R$ 1.200/m√™s).
Consumo de 14 km/L garante economia nas rotas di√°rias."
```

**Primeiro carro:**
```
"O Renault Kwid √© o primeiro carro ideal: compacto (f√°cil de estacionar),
econ√¥mico (15 km/L) e com seguro acess√≠vel. A R$ 45 mil, sobra folga
no or√ßamento para eventuais manuten√ß√µes enquanto voc√™ ganha experi√™ncia."
```

**Transporte app (Uber/99):**
```
"O Toyota Corolla 2020 est√° homologado para apps como Uber/99 e oferece
√≥tima durabilidade (crucial para alto km/m√™s). TCO de R$ 2.100/m√™s se paga
com 150-180h de trabalho mensal, deixando boa margem de lucro."
```

---

## üß™ Estrat√©gia de Testes

### Testes Unit√°rios

```python
# test_llm_justification_service.py

def test_family_context():
    """Valida justificativa para contexto familiar"""
    car = create_test_car_suv()
    profile = create_family_profile(kids=2)

    justification = service.generate_justification(
        car, profile, score=0.88, position=1, total_results=5, tco_breakdown={}
    )

    # Valida√ß√µes
    assert len(justification) > 50  # N√£o muito curto
    assert len(justification) < 500  # N√£o muito longo
    assert "fam√≠li" in justification.lower()  # Menciona fam√≠lia
    assert "R$" in justification  # Menciona valores
    assert not "emoji" in justification  # Sem emojis
```

### Testes de Integra√ß√£o

```python
# test_api_integration.py

def test_recommendations_with_llm_justifications():
    """Testa recomenda√ß√µes com justificativas LLM end-to-end"""

    # Criar perfil de teste
    profile = {
        "orcamento_min": 60000,
        "orcamento_max": 100000,
        "uso_principal": "familia",
        "tamanho_familia": 4,
        "tem_criancas": True,
        "prioridades": {"economia": 5, "seguranca": 5, "espaco": 4}
    }

    # Chamar API
    response = client.post("/api/recommendations", json=profile)

    # Validar
    assert response.status_code == 200
    results = response.json()

    for car in results["recommendations"]:
        justification = car["justificacao"]

        # Validar qualidade da justificativa
        assert justification is not None
        assert len(justification) > 100
        assert "R$" in justification  # Menciona pre√ßo ou TCO
        # Validar que n√£o √© template gen√©rico
        assert not justification.startswith("Excelente op√ß√£o")
```

### Testes de Fallback

```python
def test_fallback_on_api_failure():
    """Garante que sistema funciona mesmo se LLM falhar"""

    # Simular falha do LLM (API key inv√°lida)
    service = LLMJustificationService(api_key="invalid")

    # Deve retornar justificativa template
    justification = engine._generate_justification(
        car, profile, score, car_scores, category_score, tco_info
    )

    assert justification is not None
    assert len(justification) > 0
```

---

## ‚öôÔ∏è Configura√ß√£o

### Vari√°veis de Ambiente

**Obrigat√≥rias:**
```bash
LLM_PROVIDER=openai  # ou anthropic
OPENAI_API_KEY=sk-...  # se provider=openai
```

**Opcionais:**
```bash
LLM_MODEL=gpt-4o-mini  # default
LLM_CACHE_ENABLED=true
LLM_TIMEOUT_SECONDS=10
LLM_MAX_TOKENS=200
LLM_TEMPERATURE=0.7
```

### Modelos Recomendados

| Provedor | Modelo | Custo/1k tokens | Lat√™ncia | Qualidade |
|----------|--------|-----------------|----------|-----------|
| **OpenAI** | gpt-4o-mini | $0.00015 input<br>$0.0006 output | ~300ms | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Anthropic** | claude-3-haiku | $0.00025 input<br>$0.00125 output | ~400ms | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Local** | Llama 3.1 8B | Gr√°tis | ~800ms | ‚≠ê‚≠ê‚≠ê‚≠ê |

**Recomenda√ß√£o:** Come√ßar com **gpt-4o-mini** (melhor custo-benef√≠cio)

---

## üí∞ Estimativa de Custos

### C√°lculo por Requisi√ß√£o

**Tokens por justificativa:**
- Input prompt: ~600 tokens
- Output gerado: ~150 tokens
- Total: ~750 tokens

**Custos por recomenda√ß√£o (5 carros):**

| Provedor | Custo/req | Custo/100 req | Custo/10k req |
|----------|-----------|---------------|---------------|
| OpenAI (gpt-4o-mini) | $0.00054 | $0.054 | $5.40 |
| Anthropic (Haiku) | $0.00034 | $0.034 | $3.40 |
| Local (Llama) | $0.00 | $0.00 | $0.00 |

### Proje√ß√µes de Uso

**Cen√°rio 1: MVP/Beta (100 usu√°rios/m√™s)**
- Requisi√ß√µes: 100/m√™s
- Custo mensal: **$0.05 - $0.10**

**Cen√°rio 2: Lan√ßamento (1.000 usu√°rios/m√™s)**
- Requisi√ß√µes: 1.000/m√™s
- Custo mensal: **$0.50 - $1.00**

**Cen√°rio 3: Crescimento (10.000 usu√°rios/m√™s)**
- Requisi√ß√µes: 10.000/m√™s
- Custo mensal: **$5 - $10**

**Cen√°rio 4: Escala (100.000 usu√°rios/m√™s)**
- Requisi√ß√µes: 100.000/m√™s
- Custo mensal: **$50 - $100**

### Otimiza√ß√µes de Custo

1. **Cache agressivo:** Carros + perfil similar ‚Üí reutiliza justificativa (‚Üì 60-70%)
2. **Batch processing:** Gerar 5 justificativas em 1 chamada (‚Üì 40%)
3. **Modelo local para alta escala:** Llama 3.1 8B self-hosted (custo fixo)

---

## üîÑ Estrat√©gia de Fallback

### N√≠veis de Fallback

```
1. LLM prim√°rio (OpenAI/Anthropic) ‚Üí se falhar
   ‚Üì
2. Retry com exponential backoff (3 tentativas) ‚Üí se falhar
   ‚Üì
3. Justificativa template (c√≥digo atual) ‚Üí sempre funciona
```

### Condi√ß√µes de Fallback

- **Timeout:** > 10 segundos
- **Rate limit:** 429 da API
- **API key inv√°lida:** 401
- **Erro de rede:** Connection error
- **Output inv√°lido:** Resposta vazia ou muito curta (<50 chars)

### Logging

```python
if llm_failed:
    logger.warning(
        f"LLM fallback acionado para car_id={car.id}",
        extra={
            "error": str(e),
            "provider": self.provider,
            "model": self.model,
            "car_id": car.id,
            "profile_id": profile.id
        }
    )
```

---

## üìä M√©tricas e Monitoramento

### KPIs a Rastrear

1. **Taxa de sucesso LLM:** % chamadas que n√£o usaram fallback
2. **Lat√™ncia m√©dia:** Tempo para gerar justificativa
3. **Custo por requisi√ß√£o:** Tracking de gastos
4. **Qualidade percebida:** Feedback dos usu√°rios (Fase 2)

### Implementa√ß√£o

```python
# services/llm_justification_service.py

class LLMJustificationService:
    def __init__(self):
        self.metrics = {
            "total_calls": 0,
            "successful_calls": 0,
            "fallback_calls": 0,
            "total_latency": 0.0,
            "total_cost": 0.0
        }

    def get_metrics(self) -> dict:
        """Retorna m√©tricas agregadas"""
        return {
            "success_rate": self.metrics["successful_calls"] / self.metrics["total_calls"],
            "avg_latency": self.metrics["total_latency"] / self.metrics["total_calls"],
            "total_cost": self.metrics["total_cost"]
        }
```

---

## üìÖ Cronograma de Implementa√ß√£o

### Sprint 1 (2-3 dias)

**Dia 1:**
- [ ] Criar `llm_justification_service.py` (estrutura b√°sica)
- [ ] Criar `llm_prompts.py` (todos os templates)
- [ ] Implementar integra√ß√£o com OpenAI
- [ ] Testes b√°sicos manuais

**Dia 2:**
- [ ] Implementar cache (opcional mas recomendado)
- [ ] Implementar retry logic e error handling
- [ ] Integrar com `unified_recommendation_engine.py`
- [ ] Testes de integra√ß√£o

**Dia 3:**
- [ ] Implementar fallback strategy
- [ ] Criar testes unit√°rios completos (`test_llm_justification_service.py`)
- [ ] Documenta√ß√£o (README + vari√°veis de ambiente)
- [ ] Deploy em ambiente de staging

### Sprint 2 (Opcional - 1-2 dias)

**Melhorias:**
- [ ] Suporte para Anthropic (Claude)
- [ ] Suporte para modelo local (Llama)
- [ ] Dashboard de m√©tricas
- [ ] A/B test: LLM vs Template

---

## ‚úÖ Crit√©rios de Aceita√ß√£o

### Funcionalidade

- [x] Justificativas geradas por LLM s√£o contextuais e precisas
- [x] Fallback funciona perfeitamente se LLM falhar
- [x] Lat√™ncia adicional < 1 segundo por requisi√ß√£o
- [x] Custo por requisi√ß√£o < $0.01
- [x] Suporta pelo menos 1 provedor (OpenAI ou Anthropic)

### Qualidade

- [x] Cobertura de testes ‚â• 80%
- [x] Justificativas t√™m 100-300 caracteres (2-3 frases)
- [x] Linguagem natural, sem templates √≥bvios
- [x] Menciona detalhes espec√≠ficos do carro e usu√°rio
- [x] Sem informa√ß√µes inventadas ou incorretas

### Operacional

- [x] Sistema funciona sem LLM (fallback testado)
- [x] Logs adequados para debugging
- [x] Vari√°veis de ambiente documentadas
- [x] README com instru√ß√µes de configura√ß√£o

---

## üöÄ Como Executar Ap√≥s Implementa√ß√£o

### 1. Configurar API Key

```bash
# .env
OPENAI_API_KEY=sk-proj-...
LLM_PROVIDER=openai
LLM_ENABLED=true
```

### 2. Instalar Depend√™ncias

```bash
cd platform/backend
pip install -r requirements.txt
```

### 3. Testar

```bash
# Rodar testes
pytest tests/test_llm_justification_service.py -v

# Teste manual via API
curl -X POST http://localhost:8000/api/recommendations \
  -H "Content-Type: application/json" \
  -d '{
    "orcamento_min": 60000,
    "orcamento_max": 100000,
    "uso_principal": "familia",
    "tamanho_familia": 4,
    "tem_criancas": true,
    "prioridades": {"economia": 5, "seguranca": 5}
  }'
```

### 4. Validar Justificativas

Verificar que cada carro retornado tem:
- Campo `justificacao` preenchido
- Texto natural (n√£o template √≥bvio)
- Men√ß√£o a caracter√≠sticas espec√≠ficas
- Contextualiza√ß√£o com perfil do usu√°rio

---

## üîç Valida√ß√£o de Qualidade

### Checklist de Revis√£o de Justificativas

Para cada justificativa gerada, verificar:

- [ ] **Especificidade:** Menciona nome do carro, pre√ßo ou caracter√≠sticas √∫nicas?
- [ ] **Contextualiza√ß√£o:** Relaciona com uso principal do usu√°rio?
- [ ] **Honestidade:** Evita superlativos n√£o justificados ("excelente", "perfeito")?
- [ ] **N√∫meros:** Inclui valores concretos (TCO, consumo, pre√ßo)?
- [ ] **Tamanho:** Entre 100-300 caracteres (2-3 frases)?
- [ ] **Portugu√™s correto:** Sem erros gramaticais?
- [ ] **Sem inven√ß√µes:** Todos os dados mencionados est√£o no input?

---

## üìñ Documenta√ß√£o a Criar

1. **README_LLM.md** (guia de uso)
2. **PROMPT_ENGINEERING.md** (detalhes de prompts)
3. **COST_ANALYSIS.md** (an√°lise de custos)
4. **Atualizar CLAUDE.md** com nova funcionalidade

---

## üéØ Pr√≥ximos Passos (Fases Futuras)

Ap√≥s Fase 1 estar funcionando:

- **Fase 2:** An√°lise de contexto din√¢mica com LLM
- **Fase 3:** LangGraph para refinamento iterativo
- **Fase 4:** Aprendizado com feedback dos usu√°rios

---

## üìû Suporte

Em caso de d√∫vidas ou problemas durante implementa√ß√£o:
1. Revisar este plano
2. Consultar documenta√ß√£o dos provedores (OpenAI, Anthropic)
3. Verificar logs do backend
4. Testar com fallback desabilitado para isolar problemas

---

**Status:** üìã Plano aprovado e pronto para implementa√ß√£o
**Estimativa:** 2-3 dias de desenvolvimento + 1 dia de testes
**Custo estimado:** < $10/m√™s para MVP (100 usu√°rios)
